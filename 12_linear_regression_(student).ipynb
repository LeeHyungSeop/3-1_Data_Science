{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOQMS54kEkrq"
      },
      "source": [
        "# 데이터사이언스 (0010085001)\n",
        "\n",
        "## Exercise 12: Linear Regression\n",
        "\n",
        "In this excercise, we will implement the linear regression algorithm.\n",
        "\n",
        "- Regression models are used to describe relationship between variables by fitting a line to the observed data. \n",
        "- Regression allows you to desimtate how a dependent variable changes as the independent variable(s) change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGURm8ewQI-_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import linalg as LA\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8AQY_tZQQRo"
      },
      "outputs": [],
      "source": [
        "# 1. 데이터 생성\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n",
        "\n",
        "data = \n",
        "print(data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# regression 에 필요한 데이터만 선택\n",
        "data = \n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "-TfcDSP4x2AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Simple Linear Regression\n",
        "\n",
        "Simple Linear Regression is used to estimate the relationship between two quantitative variables."
      ],
      "metadata": {
        "id": "7p-s6V-vM0i9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_pWOtKoGCxE"
      },
      "outputs": [],
      "source": [
        "# 2. 데이터 분리\n",
        "\n",
        "# 2.1 inputs (X) 설정\n",
        "X = \n",
        "print(X.shape)\n",
        "\n",
        "# 차원 확장 (150, ) -> (150, 1)\n",
        "X = \n",
        "\n",
        "# bias 추가 (linear classification 과 동일)\n",
        "oneVector = \n",
        "print(oneVector.shape)\n",
        "\n",
        "# bias 와 기존의 X 를 결합하기\n",
        "X = \n",
        "\n",
        "# 주어진 입력을 matrix 형태로 변경\n",
        "X = \n",
        "print('inputs: ', X.shape)\n",
        "\n",
        "# 2.2 targets (Y) 설정\n",
        "Y = \n",
        "print('targets: ', Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. linear regression 수행\n",
        "\n",
        "# X, Y 를 관계짓는 weights ([w0, w1]) 을 찾기 위해서는 pseudo-inverse 계산이 필요함\n",
        "# pseudo-inverse 는 ((X_t*X)^-1)*X^T 를 통해 이루어짐\n",
        "\n",
        "# X_t 계산\n",
        "XT = \n",
        "\n",
        "# (X_t * X) 계산\n",
        "XTX = \n",
        "\n",
        "# (X_t * X)^-1 계산\n",
        "XTXINV = \n",
        "\n",
        "# ((X_t*X)^-1)*X^T -> pseudo-inverse\n",
        "XTXINVTR = \n",
        "\n",
        "# targets (Y) 와의 inner product 를 통해서 closed-form solution 을 구할 수 있음\n",
        "w_lin = np.dot(XTXINVTR, Y)\n",
        "print(w_lin)\n",
        "print(w_lin.shape)"
      ],
      "metadata": {
        "id": "fwG27f45EsY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf6ns3yPJ5BP"
      },
      "outputs": [],
      "source": [
        "# 4. 시각화\n",
        "\n",
        "intercept = \n",
        "slope = \n",
        "\n",
        "X_ = np.squeeze(np.asarray(X[:, 1]))\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_, Y, c=Y, cmap=plt.cm.Spectral)\n",
        "\n",
        "# line 생성\n",
        "x_hyperplane = np.linspace(4, 8, 10)\n",
        "y_hyperplane = \n",
        "\n",
        "plt.plot(x_hyperplane, y_hyperplane, '-')\n",
        "plt.title(\"Simple linear regression algorithm on Iris dataset\")\n",
        "plt.xlabel(\"x_1\")\n",
        "plt.ylabel(\"x_2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. error 계산\n",
        "\n",
        "# w_lin 과 입력의 inner-product 를 통해 우리의 예측 계산\n",
        "Y_pred = \n",
        "\n",
        "# Y로부터 차원 확장으로 Y_true 계산\n",
        "Y_true = \n",
        "\n",
        "# Y_pred 와 Y_true 의 residual 계산\n",
        "residual = \n",
        "\n",
        "errors_l2 = \n",
        "print('Sum of squared errors is: ', errors_l2)\n",
        "\n",
        "errors_l1 = \n",
        "print('Sum of absolute errors is: ', errors_l1)\n"
      ],
      "metadata": {
        "id": "wrO-P3oS41vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Multiple Linear Regression\n",
        "\n",
        "Multiple linear regression is used to estimate the relationship between two or more independent variables and one dependent variable."
      ],
      "metadata": {
        "id": "SRFNSVH1M67D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 데이터 분리\n",
        "\n",
        "# 2.1 inputs (X) 설정\n",
        "X = \n",
        "print(X.shape)\n",
        "\n",
        "# bias 추가 (linear classification 과 동일)\n",
        "oneVector = \n",
        "\n",
        "# bias 와 기존의 X 를 결합하기\n",
        "X = \n",
        "\n",
        "# 주어진 입력을 matrix 형태로 변경\n",
        "X = \n",
        "print('inputs: ', X.shape)\n",
        "\n",
        "# 2.2 targets (Y) 설정\n",
        "Y = data[:, 3]\n",
        "print('targets: ', Y.shape)"
      ],
      "metadata": {
        "id": "272KgjloM6dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. linear regression 수행\n",
        "\n",
        "# X, Y 를 관계짓는 weights ([w0, w1, w2]) 을 찾기 위해서는 pseudo-inverse 계산이 필요함\n",
        "# pseudo-inverse 는 ((X_t*X)^-1)*X^T 를 통해 이루어짐\n",
        "\n",
        "# X_t 계산\n",
        "XT = \n",
        "\n",
        "# (X_t * X) 계산\n",
        "XTX = \n",
        "\n",
        "# (X_t * X)^-1 계산\n",
        "XTXINV = \n",
        "\n",
        "# ((X_t*X)^-1)*X^T -> pseudo-inverse\n",
        "XTXINVTR = \n",
        "\n",
        "# targets (Y) 와의 inner product 를 통해서 closed-form solution 을 구할 수 있음\n",
        "w_lin = \n",
        "print(w_lin)\n",
        "print(w_lin.shape)"
      ],
      "metadata": {
        "id": "53RC3BtyHj9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. error 계산\n",
        "\n",
        "from numpy import linalg as LA\n",
        "\n",
        "# w_lin 과 입력의 inner-product 를 통해 우리의 예측 계산\n",
        "Y_pred = \n",
        "\n",
        "# Y로부터 차원 확장으로 Y_true 계산\n",
        "Y_true = \n",
        "\n",
        "# Y_pred 와 Y_true 의 residual 계산\n",
        "residual = \n",
        "\n",
        "print('Erros for {}-th fold'.format(k))\n",
        "errors_l2 = LA.norm((Y_pred - Y_true), ord=2)\n",
        "print('Sum of squared errors (MSE) is: ', errors_l2)\n",
        "\n",
        "errors_l1 = LA.norm((Y_pred - Y_true), ord=1)\n",
        "print('Sum of absolute errors (MAE) is: ', errors_l1)\n"
      ],
      "metadata": {
        "id": "TvuPG8qs7pY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. K-fold cross validation"
      ],
      "metadata": {
        "id": "9kNp0uokyMeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = datasets.load_iris()\n",
        "X = data.data\n",
        "Y = data.target\n",
        "\n",
        "# 교차검증 횟수만큼 for loop 수행\n",
        "for k in range (0,10):\n",
        "    # 학습 (training) / 평가 (test) 데이터 구분\n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "    X_train, X_test, _, _ = \n",
        "    \n",
        "    # targets (Y) 설정\n",
        "    Y_train = X_train[:, 3]\n",
        "    Y_test = X_test[:, 3]\n",
        "\n",
        "    # inputs (X) 설정\n",
        "    X_train = X_train[:, 0]\n",
        "    X_test = X_test[:, 0]\n",
        "    \n",
        "    # 차원 확장 \n",
        "    X_train = np.expand_dims(X_train, axis=1)\n",
        "    X_test = np.expand_dims(X_test, axis=1)\n",
        "\n",
        "    # bias 추가 (linear classification 과 동일)\n",
        "    oneVector_train = np.ones((X_train.shape[0], 1))\n",
        "    oneVector_test = np.ones((X_test.shape[0], 1))\n",
        "\n",
        "    # bias 와 기존의 X_train 를 결합하기\n",
        "    X_train = np.concatenate((oneVector_train, X_train), axis=1)\n",
        "    X_test = np.concatenate((oneVector_test, X_test), axis=1)\n",
        "\n",
        "    ######################\n",
        "    ### training phase ###\n",
        "    ######################\n",
        "\n",
        "    # 주어진 입력을 matrix 형태로 변경\n",
        "    X_train = np.asmatrix(X_train)\n",
        "    \n",
        "    # Training the model using Linear Regresion \n",
        "    XT = np.transpose( X_train )\n",
        "    XTX = np.matmul( XT, X_train )\n",
        "    XTXINV = np.linalg.inv( XTX )\n",
        "    XTXINVTR = np.dot( XTXINV, XT )\n",
        "    \n",
        "    # k 번째 검증에 대한 w_lin 계산\n",
        "    w_lin = np.dot( XTXINVTR, Y_train )    \n",
        "\n",
        "    w_lin_ = np.transpose(w_lin)\n",
        "  \n",
        "  \n",
        "    ######################\n",
        "    ##### test phase #####\n",
        "    ######################\n",
        "\n",
        "    # w_lin 과 X_test의 inner-product 를 통해 우리의 예측 계산\n",
        "    Y_pred = np.matmul(X_test, w_lin_)\n",
        "\n",
        "    # Y로부터 차원 확장으로 Y_true 계산\n",
        "    Y_true = np.expand_dims(Y_test, axis=1) \n",
        "\n",
        "    # Y_pred 와 Y_true 의 residual 계산\n",
        "    residual = Y_pred - Y_test\n",
        "\n",
        "    print('Erros for {}-th fold'.format(k))\n",
        "    errors_l2 = LA.norm((Y_pred - Y_true), ord=2)\n",
        "    print('Sum of squared errors (MSE) is: ', errors_l2)\n",
        "\n",
        "    errors_l1 = LA.norm((Y_pred - Y_true), ord=1)\n",
        "    print('Sum of absolute errors (MAE) is: ', errors_l1)"
      ],
      "metadata": {
        "id": "pZA2k982xm24"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}